---
layout: paper
title: "Gaze Complements Control Input for Goal Prediction During Assisted Teleoperation"
invisible: true
---
<head>
<style>
* {
  box-sizing: border-box;
}

#myInput {
  background-position: 10px 10px;
  background-repeat: no-repeat;
  width: 100%;
  font-size: 100%;
  padding: 12px 20px 12px 40px;
  border: 1px solid #ddd;
  margin-bottom: 12px;
}

#myTable, #myTableA {
  border-collapse: collapse;
  width: 100%;
  border: 1px solid #ddd;
  font-size: 100%;
}

#myTable th, #myTable td, #myTableA th, #myTableA td {
  text-align: left;
  padding: 12px;
}

#myTable tr, #myTableA tr {
  border-bottom: 1px solid #ddd;
}

#myTable tr.header, #myTable tr:hover, #myTableA tr.header, #myTableA tr:hover {
  background-color: #f1f1f1;
}


#eventcounter1 a {
    font-size: 12px;
    color: #ffffff;
    display: block;
}

#eventcounter1 a:hover {
    text-decoration: none;
}

#eventcounter2 a {
    font-size: 12px;
    color: #ffffff;
    display: block;
}

#eventcounter2 a:hover {
    text-decoration: none;
}

</style>
</head>

<table width = "95%" style="padding-left: 15px; margin-left: auto; margin-right: 10px;">
<tr><td style = "vertical-align: top; padding-right: 25px;" rowspan="2">
<span style="color:black; font-size: 110%;"><i>
Reuben M Aronson <span style="color:gray; font-size: 85%">(Carnegie Mellon University)</span><span style="color:gray; font-size: 100%">,</span><br>
Henny Admoni <span style="color:gray; font-size: 85%">(Carnegie Mellon University)</span>
</i></span>
</td>

<td style="text-align: right;"><a href="http://www.roboticsproceedings.org/rss18/p025.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a><br></td>
</tr>
<tr>
<td style="color:#777789; text-align:right; font-size: 75%; margin-right:10px;">Paper&nbsp;#025</td>
</tr>
</table>

<table width="80%" style="margin-top: 20px; margin-left: auto; margin-right: auto;">
  <tr>
    <td style="text-align:center;">4. Short talks</td>
  </tr>
</table>
<br>


### Abstract
Shared control systems can make complex robot teleoperation tasks easier for users. These systems predict the user's goal, determine the motion required for the robot to reach that goal, and combine that motion with the user's input. Goal prediction is generally based on the user's control input (e.g., the joystick signal). In this paper, we show that this prediction method is especially effective when users follow standard noisily optimal behavior models. In tasks with input constraints like modal control, however, this effectiveness no longer holds, so additional sources for goal prediction can improve assistance. We implement a novel shared control system that combines natural eye gaze with joystick input to predict people's goals online, and we evaluate our system in a real-world, COVID-safe user study. We find that modal control reduces the efficiency of assistance according to our model, and when gaze provides a prediction earlier in the task, the system's performance improves. However, gaze on its own is unreliable and assistance using only gaze performs poorly. We conclude that control input and natural gaze serve different and complementary roles in goal prediction, and using them together leads to improved assistance.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<table width="100%" style="margin-top:40px;">
<tr>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/024/">
<img src="{{ site.baseurl }}/images/previous_paper_icon.png"
       alt="Previous Paper" width = "142"  height = "90"/> 
</a> </td>
<td style="text-align: center;"><a href="{{ site.baseurl }}/program/papers">
<img src="{{ site.baseurl }}/images/overview_icon.png"
       alt="Paper Website" width = "142"  height = "90"/> 
</a> </td>
    <td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/026/">
    <img src="{{ site.baseurl }}/images/next_paper_icon.png"
        alt="Next Paper" width = "142"  height = "90"/>
    </a></td>
'</tr>
</table>
